{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cohen's kappa score\n",
    "\n",
    "Computes the kappa socre for two sets of annotated data stored in a .txt file. The program assumes that the annotations are categorical.\n",
    "\n",
    "If the kappa score is greater that 0.8, then it is considered excellent agreement, 0.6 to 0.8 is substantial agreement, and 0.4 to 0.6 iss considered moderte agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import cohen_kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_annotations_from_file(file_path):\n",
    "    \"\"\"\n",
    "    Read annotations from a text file and return as a list of labels.\n",
    "\n",
    "    :param file_path: The path to the text file containing annotations.\n",
    "    :return: List of labels.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = [line.strip() for line in file]\n",
    "\n",
    "    # Extracting labels from the last space-separated element of each line\n",
    "    annotations = [line.split()[-1] for line in lines if len(line.split()) > 1]\n",
    "\n",
    "    return annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_observed_agreement(annotations1, annotations2):\n",
    "    \"\"\"\n",
    "    Compute observed agreement for two sets of annotations.\n",
    "\n",
    "    :param annotations1: List of annotations from the first annotator (list of categorical labels).\n",
    "    :param annotations2: List of annotations from the second annotator (list of categorical labels).\n",
    "    :return: Observed agreement.\n",
    "    \"\"\"\n",
    "    assert len(annotations1) == len(annotations2), \"Annotations lists must have the same length.\"\n",
    "\n",
    "    num_agreements = sum(1 for a1, a2 in zip(annotations1, annotations2) if a1 == a2)\n",
    "    total_data_points = len(annotations1)\n",
    "    return num_agreements / total_data_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_agreement_by_chance(annotations1, annotations2):\n",
    "    \"\"\"\n",
    "    Compute agreement by chance (expected agreement) for two sets of annotations.\n",
    "\n",
    "    :param annotations1: List of annotations from the first annotator (list of categorical labels).\n",
    "    :param annotations2: List of annotations from the second annotator (list of categorical labels).\n",
    "    :return: Agreement by chance.\n",
    "    \"\"\"\n",
    "    assert len(annotations1) == len(annotations2), \"Annotations lists must have the same length.\"\n",
    "\n",
    "    labels = set(annotations1 + annotations2)\n",
    "    label_counts_annotator1 = {label: annotations1.count(label) for label in labels}\n",
    "    label_counts_annotator2 = {label: annotations2.count(label) for label in labels}\n",
    "\n",
    "    agreement_by_chance = sum(\n",
    "        (label_counts_annotator1[label] / len(annotations1)) * (label_counts_annotator2[label] / len(annotations2))\n",
    "        for label in labels\n",
    "    )\n",
    "    return agreement_by_chance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observed Agreement: 0.9837\n",
      "Agreement by Chance: 0.7617\n",
      "Cohen's Kappa Score: 0.9315\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example usage:\n",
    "    file_path_annotator1 = \"/Users/ellyzamaripapas/Code/NER_Project/data/annotated_data/final-1.txt\"\n",
    "    file_path_annotator2 = \"/Users/ellyzamaripapas/Code/NER_Project/data/annotated_data/final-2.txt\"\n",
    "\n",
    "    annotations_annotator1 = read_annotations_from_file(file_path_annotator1)\n",
    "    annotations_annotator2 = read_annotations_from_file(file_path_annotator2)\n",
    "\n",
    "    observed_agreement = compute_observed_agreement(annotations_annotator1, annotations_annotator2)\n",
    "    agreement_by_chance = compute_agreement_by_chance(annotations_annotator1, annotations_annotator2)\n",
    "    kappa_score = cohen_kappa_score(annotations_annotator1, annotations_annotator2)\n",
    "\n",
    "    print(f\"Observed Agreement: {observed_agreement:.4f}\")\n",
    "    print(f\"Agreement by Chance: {agreement_by_chance:.4f}\")\n",
    "    print(f\"Cohen's Kappa Score: {kappa_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
